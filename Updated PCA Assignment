---
title: 'Exercise: Principal Component Analysis'
output:
  word_document: default
  pdf_document: default
  html_document: default
---

**Course Instructor**: Dr. Suborna Ahmed
**Student Name**: 
**Total Points**: 10
**Due**: Feb 23 2026

**Submission**:

1. Write your code and answers in this RMD file
2. Knit your completed RMD file to WORD document
3. Check and upload your WORD document to Exercise 4 on Canvas

------------------------------------------------------------------------

1. To become familiar with Principal Components Analysis (PCA) to transform the data (one set of variables) into new variables (the principal components) that can be used for further analyses.

2. To select a subset of x-variables based on the PCA results to use in further analysis.

3. Alternatively, to select a subset of these principal components for further analysis. These selected principal components may have meaning with regards to combinations of the x-variables.

### General Description

We have data on body measures for 32 individuals at different ages (the X matrix) along with a measure of body fat (`fat`). The X variables are: length at ages 9 and 18 (`lg9` and `lg18`); weight at ages 2, 9 and 18 (`wt2`, `wt9`, `wt18`); strength at ages 9 and 18 (`st9`, `st18`), and height at ages 2, 9, and 18 (`ht2`, `ht9`, `ht18`).

The overall objective is to predict the y-variable `fat` from the x-variables. However, there are 10 x-variables, and these x-variables are strongly correlated.

You will use the PCA results in __two different ways__:

1. In the __first approach__, you will use PCA results to reduce the number of X variables and
then run a regression with `fat` as the y-variable on the reduced set of X variables.

2. In the __second approach__, you will use the PCA results to select a number of principal
components, you will try to label these PC’s, and then use them to predict `fat`.

### Process

1. For this analysis, the data have been provided as `pca.txt` along with R code within this Rmd file.

2. You will need to run the R code by clicking on the Knit button in RStudio to run the entire file or
run each individual code chunk and get the outputs. You can find the 
`.html` and word `.docx` rendering of this Rmd file in the same working directory if you use Knit.

3. The outputs will include:
    + The PCA on variables other than `fat`;
    + A regression analysis of `fat` versus a reduced set of the X variables (first
approach); and
    + A regression of `fat` versus a reduced set of principal components (second
approach).

4. You will be answering **10 questions**. As you answer the questions, paste in the relevant R outputs to
support your answers. Question 1 - 5 (Part I: First Approach) will appear first, question 6 - 9 
(Part II: Second Approach) and question 10 (Part III: Overall Summary) will appear at the 
bottom of this file. There will be some *questions to think about* to help guide you through the code outputs, 
and you do not need to answer them, but try to give them some thoughts.

----

### R CODES

**NOTE:** *PCA.R: Uses data for Exercise 2 from an outside text file comma delimited 
(.csv) file converted from a EXCEL file OR using a tab or space delimited 
.txt file. (See Exercise 2 PCA.docx for a description of the data).
`fat` is not used in the PCA.  However, the PCA is used to select variables
to predict `fat` in a subsequent regression analysis, also included in this 
R code.*

#### PART 1: Set up

```{r}
rm(list=ls(all=TRUE)) # remove any objects from old R sessions

# set the directory for the data (and any files saved)
#setwd("C:\\Data\\FRST531\\yFRST531 2020-21\\Module 4 Principal Components Analysis\\Exercise 2 PCA")
#dir()
```

#### PART 2: Read the data and put it in the object `fatdat`

```{r}
fatdat <- read.table("~/Downloads/pca.txt", header=T)
#fatdat<-read.csv("pca.csv", header=T) # would read a comma delimited file this way instead.
```

#### PART 3: Print out some aspects of the data to just check it out

```{r}
str(fatdat)   # what is in this file?  Uses $ before each item in the file
names(fatdat)  # print out the column (i.e., variable) names
dim(fatdat)    # number of rows and columns
```

#### PART 4: Take a look at parts of the data via indexing and other ways
```{r}
head(fatdat)  # print out the first few lines of data
fatdat[1,]    # print out the first line of data
fatdat[,1]    # print out the first column of data
fatdat$wt2    # print out the wt2 values
fatdat[1:3,1:2]  # print out lines 1 to 3 and columns 1 and 2
```

#### PART 5:  Get basic stats including min, max, mean, std.dev. 
Do this by using a function from the package `pastecs`. To do this you need to 
load `pastecs` first!

```{r}
library(pastecs) # won't work if you have not installed the package first!!
options(digits=2) # set the number of decimals to be 2
stat.desc(fatdat) # get some basic stats.
```

#### PART 6: Select the x-variables from `fatdat`.
**NOTE:** Three different ways are shown here, they all do the same thing!

Option 1:

```{r}
xdata<-fatdat[,1:10] # save the x variables (columns 1 to 10)
```

Option 2:

```
xdata <- subset(fatdat, select=-fat) # -fat means don't include it.
```

Option 3:

```
xdata <- subset(fatdat, select=c(wt2,ht2,wt9,ht9,lg9,st9,wt18,ht18,
lg18,st18))
```

Verify that we extract the correct columns:
```{r}
names(xdata)
```
#### PART 7: Scatterplot matrix for pairs of x variables, and correlations amongst x-variables

```{r}
pairs(xdata, pch = 1,col="blue")  # huge graph as there are 10 x-variables.

round(( cor(xdata)),3) # round( ) is used to round to the nearest 3 decimal places.
```
Looks like correlation values! 

#### PART 8: Get the PCA using the correlation matrix via `cor=TRUE`

```{r}
xdata.pca <- princomp(xdata, cor=TRUE,scores=TRUE)

class(xdata.pca) # confirm that this gave us a princomp object.
str(xdata.pca) # what is in this list object?  Lots of stuff

methods(class=princomp)  # show the information we can ask for from the princomp object
```
We can get `biplot.princomp*`  `plot.princomp*`   `predict.princomp*` `print.princomp*`  
`summary.princomp*` from the `princomp` object.

### PART 9: Interpret the PCA. What do we want?
1. The eigenvalues (and cumulative percentages) along with the eigenvectors

#calculate the square of the standard deviations

2. Get a Scree plot of the the eigenvalues

3. Get the correlations between the new Principal Components and the 
x-variables

4. You will also need the Principal Component scores to answer the questions in the exercise.

**NOTES:** This R package reports the square root of them, 
so the standard deviations of each Component rather than the variance of each component. Also, the "Loadings" here are the eigenvectors (the multipliers to get the Principal Component Scores. They are not correlations between the new principal components and the x-variables. Finally, we need to force it to display all values otherwise some eigenvector elements will not be displayed.

```{r}
summary(xdata.pca, loadings=TRUE,cutoff=0.0)  # cutoff insures all are displayed
```

# loadings are the eigenvectors; multipliers  

Want a nicer table? Calculate the variances, percent variances, and cumulative 
percent variances and put these together using column bind.
`round()` these using `d=2` (i.e., two decimal places)

```{r}
table<-round(cbind("Variance"=xdata.pca$sdev^2,
 "%"=100*xdata.pca$sdev^2/sum(xdata.pca$sdev^2),
  "Cumulative %"=100*cumsum(xdata.pca$sdev^2)/sum(xdata.pca$sdev^2)),
   d=2)
```

```{r, percent-var}
knitr::kable(table) # display the table inside the html file, not important.
```

Do a bit of checking on the eigenvalues -- they sum to the number of x-variables since the correlation matrix was used (same as using standardized x's).

```{r}
covPC1<-round((cov(xdata.pca$scores)),3) # Show the covariance matrix for the Components
covPC1 # variances on the diagonal, and then all covariances are 0.
Lambda<-as.matrix(covPC1)
class(Lambda)
sum(diag(Lambda)) # The sum of the variances of the Components = 10 since p=10.
```

Note: there shouldnt be any correlation bewteen Component 1 and 2. 

**QUESTION 1: Determine the proportion of sample variance explained by the first three principal components. 
Is this enough? Too many?  You may find the scree plot useful here also. 
Provide the evidence from the R analysis and use the evidence to support your answer.**

```{r}
# provide your R codes here (if possible)

eig<-xdata.pca$sdev^2 #eigenvalues
eig[1:3] #selecting the first three eigenvalues
sum(eig[1:3]) #Summing the first 3 eigen values

#screeplot
plot(xdata.pca$sdev^2, type="b", pch=16, xlab="Component number", 
ylab="Eigenvalue", main="Scree plot for x-variables of fat data")

```

***Question 1 Answer***

**The first three principal components explain approximately 83% of the total sample variance (PC1 = 52%, PC2 = 18%, PC3 = 12%), as calculated from the squared standard deviations of the PCA output and checked against the cumulative proportions.** 

**83% proportion seems sufficient to reduce dimensions to the first three components. The first two principal components explain approximately 70% of the total variance alone. While this could justify only keeping only the first two components, including the third component  does increase the explained variance by a sizable margin (12%) and still maintains keeping a relatively small number of components (3 out of 10).**

**The scree plot help to support this. The plot shows a steep decline in variance explained from PC1, and then PC2 and PC3 decline moderately until a clear flattening of the curve occurs thereafter. This "elbow" indicates that the first three components offer the most power, with diminishing returns from the components PC3. Together, the cumulative variance explained and the scree plot suggest that retaining three principal components captures the majority of the meaningful variation in the data, with the rest contributing only minimally to explanatory power.** 

----

Two different scree plots here. Chose the one you like better.

```{r}
screeplot(xdata.pca) # NOTE that this does show the PC variances.
```

Get a different screeplot by controlling what it looks like. We need to use sdev^2 (that is the variance) of each component.
This is in the `xdata.pca` object as `sdev`. 

```{r}
plot(xdata.pca$sdev^2, type="b", pch=16, xlab="Component number", 
ylab="Eigenvalue", main="Scree plot for x-variables of fat data")
```

**QUESTION 2: Using the Jackson (1993) article and the broken stick method described in that article, determine how many principal components to retain. Does this support using the first
three principal components or not? Explain your answer also.**

```{r}
# provide your R codes here (if possible)

# observed eigenvalues
eig <- xdata.pca$sdev^2
p <- length(eig)

# generate broken stick expected proportions for p components
bs_prop <- sapply(1:p, function(k) sum(1/(k:p)) / p)

# convert to expected eigenvalues on the same scale as eig
bs_eig <- bs_prop * sum(eig)

# which PCs to retain by broken stick rule
retain <- which(eig > bs_eig)
retain #shows a value of 1
length(retain)

# quick check table
cbind(PC = 1:p, eig = eig, broken_stick = bs_eig, keep = eig > bs_eig)

# plot observed vs broken stick
plot(eig, type = "b", xlab = "Component", ylab = "Eigenvalue")

keep <- eig > bs_eig
keep #True/False table showing which componetns to drop 
which(keep)
sum(keep)

keep

bs_prop

out <- data.frame(
  PC = 1:p,
  eigenvalue = eig,
  broken_stick = bs_eig,
  keep = eig > bs_eig
)
out #creates a data table 

plot(eig, type = "b", xlab = "Component", ylab = "Eigenvalue")


```

***Question 2 Answer***

**When using the broken stick method, only the first principal component
(5.239) exceeded its broken stick expectation (2.93). The second (1.93)
and third (1.43) components were both smaller than their respective
broken stick expectations. Therefore, according to the broken stick
method, only one principal component should be retained (the first one).
This does not support retaining the first three components. The broken
stick method suggests that most of the meaningful structure in the data
is captured by the first component alone.**


----

**Hint for For #3**: Get the correlations beween the scaled x variables (sincethe correlation matrix of X was used for the PCA) and the new principal components.

**Questions to think about:**

A. Which x-variables are related to the Principal Components that 
you will keep? In what way - or +?

B. What "cutoff" correlation did you use (e.g., |r|>0.5 ?) and why?

C. Finally, can the Principal Components be labeled given which x's
they relate to and also in what way they relate?

**QUESTION 3: What X variables are related to the first three components. Show the evidence in the R outputs and indicate what “cutoff” value you are using to decide which x-variables are related to each component.**

```{r}
# provide your R codes here (if possible)

loadings <- as.matrix(xdata.pca$loadings[, 1:3]) #extract the PCA loadings from the larger dataframe

sdev <- xdata.pca$sdev[1:3] #extract the first 3 component standard deviations

cor_x_pc <- sweep(loadings, 2, sdev, FUN = "*") #compute correlations; Multiply each column of loadings by the corresponding component standard deviation values

cor_x_pc

cutoff <- 0.4 

apply(cor_x_pc, 2, function(x) names(x)[abs(x) >= cutoff])

cor_table <- data.frame(
  variable = rownames(cor_x_pc),
  PC1_cor = cor_x_pc[,1],
  PC2_cor = cor_x_pc[,2],
  PC3_cor = cor_x_pc[,3],
  PC1_related = abs(cor_x_pc[,1]) >= cutoff,
  PC2_related = abs(cor_x_pc[,2]) >= cutoff,
  PC3_related = abs(cor_x_pc[,3]) >= cutoff
) #regression table

#ways of representing and visualizing results: 

cor_table

biplot(xdata.pca, choices = c(1, 2), scale = 0)

biplot(xdata.pca, choices = c(1, 3), scale = 0)

loadings <- as.matrix(xdata.pca$loadings[, 1:3])

loadings



```

***Question 3 Answer***

**We used a cutoff of r \> 0.4 to decide which variables are most strongly related to each component. We thought a cutoff of 0.4 was a good threshold here, which would represent the most reasonably  meaningful (largest) associations. The correlation results seem to justify this, since PC1 correlations hover between 0.6 - 0.9. A 0.4 cutoff thus seemed to cleanly filter out the strongest associations.**

**For PC1, all variables (wt2, ht2, wt9, ht9, lg9, st9, wt18, ht18, lg18, and st18) have correlations greater than 0.4 and are positive. This shows that PC1 is related to all body size measurements. Upon interpretation, this might represent patterns for overall growth or general size, with larger values linked to larger measurements across ages. For example, for PC2, st9, ht18, st18 (r = 0.57, 0.41, 0.46) is positively related and lg9, wt18, and lg18 (r= -0.47, -0.44, -0.59 respectively), particularly at older ages.** 

**For PC3, wt2 (0.54), ht2 (0.53), wt9 (0.45), ht9 (0.40), lg9 (0.41), st9 (−0.53), and st18 (−0.41) are related to the component. This suggests that PC3 contrasts early body size with later strength measurements.**

**Overall, these variable indicate moderate to strong relationships between the original variables and the principal components, with PC1 exerting the largest influence, consistent with the dominant size-related component.**


----

Correlations between scaled x's and components

```{r}
# Get the standardized xdata first
Xscaled<-scale(xdata, center = TRUE, scale = TRUE)
round(cor(xdata.pca$scores, Xscaled,),3)
```

Graphs of correlations might be easier to interpret than the 
tables of correlations. Graph the correlations for components 1 and 2
over the Principal Scores to see which x's relate to each component.

```{r}
biplot(xdata.pca)
```
# PCA Scores and Loadings
This biplot shows both the individual observations (as gray dots) and the original variables (as blue arrows) on the first two principal components. The direction and length of arrows indicate how much a variable contributes to the PCs. Arrows pointing in the same direction indicate positively correlated variables, while those at right angles indicate uncorrelated variables.

```{r}
# Extract PCA scores and loadings
scores <- xdata.pca$scores[, 1:2]      # PC1 and PC2 scores
loadings <- xdata.pca$loadings[, 1:2]  # Loadings for PC1 and PC2

# Scale loadings for better display
loadings_scaled <- loadings * sqrt(xdata.pca$sdev[1:2]) * 2

# Create empty plot with appropriate limits
plot(scores,
     xlab = "PC1", ylab = "PC2",
     main = "Biplot of Principal Components",
     pch = 19, col = "gray40", cex = 0.8)
abline(h = 0, v = 0, col = "gray")

# Add arrows for variables
arrows(0, 0, loadings_scaled[,1], loadings_scaled[,2],
       length = 0.1, col = "blue", lwd = 2)

# Add variable names
text(loadings_scaled[,1], loadings_scaled[,2],
     labels = rownames(loadings_scaled),
     col = "blue", cex = 0.9, pos = 4)

```


```{r}
xdata.pca$scores
```


# Observation 1, using first principal component, the data is actually being converted to a prinvipal component matches. So this dataset is converted, now we're looking at principal components. 



**QUESTION 4: Repeat #3 using the first two principal components instead.**

```{r}
# provide your R codes here (if possible)

loadings <- as.matrix(xdata.pca$loadings[, 1:2]) #extract the PCA loadings from the larger dataframe

sdev <- xdata.pca$sdev[1:2] #extract the first two component standard deviations

cor_x_pc2 <- sweep(loadings, 2, sdev, FUN = "*") #compute correlations; Multiply each column of loadings by the corresponding component standard deviation values

cor_x_pc2

cutoff <- 0.4 #apply the same cutoff used earlier

apply(cor_x_pc2, 2, function(x) names(x)[abs(x) >= cutoff])

#create the new table
cor_table2 <- data.frame(
variable = rownames(cor_x_pc2),
PC1_cor = cor_x_pc2[,1],
PC2_cor = cor_x_pc2[,2],

#create the new regression table 
PC1_related = abs(cor_x_pc2[,1]) >= cutoff,
PC2_related = abs(cor_x_pc2[,2]) >= cutoff
) 

#ways of representing and visualizing results: 
cor_table2

biplot(xdata.pca, choices = c(1, 2), scale = 0)

loadings2 <- as.matrix(xdata.pca$loadings[, 1:2])

loadings2

```

Class note: if they are not 90 degrees, they are correlated with each other. 
```{r}
# Extract PC1 and PC2 scores
pc_scores <- xdata.pca$scores[, 1:2]

# Correlation between PC1 and PC2 scores
cor(pc_scores[,1], pc_scores[,2])
```
This resulting correlation value is effectively zero, indicating that PC1 and PC2 are orthogonal and sitting at 90 degrees of each other. 


***Question 4 Answer***

**Using the same cutoff of 0.4, this model using only the first two principal components makes interpretation a bit clearer. For PC1, all body size variables across ages (2, 9, and 18) are positively and strongly correlated, akin to the earlier model iteration. This reinforces the idea that PC1 represents overall body size or general growth, since it captured variation in the other variables regardless of age.** 

**This new model, meanwhile, showed a different and clearer interpretation of PC2. st9, ht18, and st18 are positive relationship with PC2, but lg2, wt18, and lg18 are negatively correlated. The age-2 and some age-9 weight/height variables are not strongly related to PC2. Therefore, PC2 appears to capture differences in body structure or proportional growth at later ages, rather than overall size.**

----

#### PART 10: Use the PCA results and then develop a model to predict fat

**MODEL 1**: USE a **subset** of x variables
Here, only `ht9`, `wt9` and `lg9` are included. Right ones? You will 
be deciding in this exercise.

```{r}
lm.M1<-lm(fat~ht9+wt9+lg9,data=fatdat) #fit the model
lm.M1  # get the estimated coefficients for the model
anova(lm.M1)   # get the Anova table for the model, including the F test
summary(lm.M1) # get t-tests and some fit statistics for the model
sum1<-summary(lm.M1)  # save the summary statistics to use later. 
```

Note: probably going to reject the null hypothesis. 

R-squared represents the variance explained by the model. It may not reflect your prediction. If 80%, that means 20% unexplained and could be close. But sometimes we have to work with other thresholds. 60% could be good enough based on the dataset. 

Findings: A multiple linear regression of fat on height, weight, and length showed that weight and height were significant predictors of fat content, while length was not. The model explained approximately 59% of the variance in fat (adjusted R² = 0.54) and was highly significant overall (F₃,₂₈ = 13.27, p < 0.001). Differences between the ANOVA table and coefficient t-tests reflect the use of sequential versus partial sums of squares, indicating shared variance among predictors.


Add the M1 fitted and residual values to `fatdat`

```{r}
fatdat$yhat.M1<-fitted(lm.M1)  # add the estimated y values to fatdat
fatdat$resid.M1<-resid(lm.M1)  # add the residuals to fatdat
fatdat$std.resid.M1<-resid(lm.M1)/sum1$sigma # standarize the residuals.
names(fatdat)
head(fatdat)
```

Note: prediction implies some unexplained areas. We are getting close to the actual value, but not the exact prediction. If you don't get unexplained area, you are probably overfitting. 


Get diagnostic plots

```{r}
# run from here...
str(fatdat$yhat.M1) #confirming vectors are numeric 
str(fatdat$resid.M1)

fatdat$yhat.M1  <- as.numeric(fatdat$yhat.M1)
fatdat$resid.M1 <- as.numeric(fatdat$resid.M1)

dev.new()

par(mfrow = c(2,2), mai = c(0.6,0.6,0.6,0.6), cex = 0.7)

plot(fatdat$yhat.M1, fatdat$resid.M1,
     main = "Model 1, Residual Plot",
     xlab = "yhat", ylab = "residual")

abline(h = 0, lty = 2)


minfat<-min(fatdat$fat)
maxfat<-max(fatdat$fat)

plot(fatdat$fat,fatdat$yhat.M1, main="Model 1, Fitted line plot",
  ylab="yhat", xlab="fat",ylim=c(minfat,maxfat),xlim=c(minfat,maxfat))
abline(0,1)

qqnorm(fatdat$std.resid.M1, main="Model 1, Normality plot")
abline(0,1)

hist(fatdat$resid.M1, breaks =8 , density=10,col="green", border="black",
main="Model 1, Error Distribution") 
#par(mfrow=c(1,1),mai=c(1.0,1.0,1.0,1.0),cex=1.0)

# to here....
```

Class Notes: look at the errors you have, in Model 1, the average should be close to 0.If its not close to 0, there are issues in the model. You have to add more observations, or need to transform your variable. In the fitted plot line, we want everything on the 45 degree line. If not, we have unexplained data and are not predicting well. In normality plot, you want everything to be on the 45 angle. For the error distrubution, this is kind of creating a normal distribution. If the dist is normal, the errors are not creating difficulty. If skewed too much, the errors are probably large (need to find out where and why). 


**QUESTION 5. Based on your answers to Questions #3 and #4, and the first regression analysis using `fat` versus the reduced set of X variables:**

**a. Would you agree with the choice of the reduced set of X variables that are used in the regression for `fat`? Why or why not? If you do not agree, indicate what x-variables you would have selected and why you would have selected these,
but continue with interpreting the regression outputs using the R code
provided.**

***Answer***
**Based on the PCA results and insights from Questions 3 and 4, a reduced set of X variables representing overall body size seems justified, as the first principal component explained approximately 52% of the total variance and showed strong correlations with all size-related variables (|r| ≈ 0.55–0.92). The selection of ht9, wt9, and lg9 is therefore reasonable, as these variables represent height, weight, and length and align with the dominant size gradient from PC1.**

**At the same time, the regression results indicate that while both height and weight are significant predictors of fat (ht9: t = −3.32, p = 0.0025; wt9: t = 3.34, p = 0.0024), length does not contribute additional explanatory power once height and weight are included (lg9: t = −0.12, p = 0.91). The full model explained approximately 59% of the variance in fat (adjusted R² = 0.54). Thus, although the initial reduced set of variables were supported by PCA, the regression analysis here suggests that model excluding lg9 would not result in a substantial loss of explanatory power, and would produce a cleaner model with fewer predictors.** 


**b. What was the $R^2$ value for this regression with the reduced set of variables? What does this mean? Is this a good model? NOTE: $R^2$ is the proportion of variance of `fat` explained by the x-variables.**

***Answer***
**The regression using the reduced set of variables (ht9, wt9, and lg9) had an R² of 0.587 (adjusted R² = 0.543), indicating that approximately 59% of the variance in fat was explained by the model. This represents a reasonably strong fit for biological data, particularly given the small number of predictors and the inherent variability in the response. The model was also highly significant overall (F₃,₂₈ = 13.27, p < 0.001), suggesting that the reduced set of variables still captures a substantial portion of the variation in fat, although some unexplained variance remains.**

**c. Look at the fitted line plot. Is the relationship between the predicted and measured `fat` a “good” relationship? NOTE: A good relationship would show a linear trend between the predicted versus measured `fat` (not a curve!) and low variability around this line.**

***Answer***
**In the residual plot, points are scattered more-or-less evenly above and below the zero line, indicating that the model is not over or under-predicting fat values. Therefore, there doesn't seem to be a need to add additional observations or transform any of the response variables based on this residual bias.**

**The fitted line plot, meanwhile, shows a fairly linear relationship between predicted and measured fat values. While not all points fall exactly on the 45° line, the outliers are also not significantly distant from the line. This would indicate that the model is capturing a mostly linear trend from the data, and although some unexplained variation remains, this seems expected given the model R² of ~0.59.**

**The Q–Q plot shows that the standardized residuals largely align with the 45° reference line, with only minor deviations in the tails. This would suggest that the residuals are distributed normally and that the normality assumption of the linear model is reasonably met.** 

**Finally, the error histogram shows a mostly bell-shaped distribution, centered near zero. There is no strong skewness or extreme outliers, indicating that the errors are not dominated by a small number of large residuals and are unlikely to be creating major difficulties.** 

**Taken together, the diagnostic plots indicate that Model 1 performs well. It is not perfect but does not seem to require additional transformations or additions. Although some unexplained variance remains, the diagnostic plots do not suggest major violations, so the predictive relationship between measured and predicted fat seems adequate for this dataset.**


*NOTE*: Since the X’s are correlated, you cannot interpret the coefficients of the resulting fitted model.


____

**MODEL 2**: USE a __subset__ of PC scores. Use `PC1`, `PC2` and `PC3`.

Make a dataframe of x-variables, first three PC scores and y variables.


```{r}

#combine fatdat with the first 3 PCA scores into a new df, and double check
regdata2<-cbind(fatdat,xdata.pca$scores[,1:3])
head(regdata2)

#fit the regression model
lm.M2<-lm(fat~Comp.1+Comp.2+Comp.3,data=regdata2) 
lm.M2  

# get the Anova table for the model, including the F test, looking at how the PCs contribute to fat 
anova(lm.M2)  
summary(lm.M2) 
sum2<-summary(lm.M2) # save summary statistics to use later on

#add the residuals, then standardize them via standard error, then double check
regdata2$yhat.M2<-fitted(lm.M2)  
regdata2$resid.M2<-resid(lm.M2)  
regdata2$std.resid.M2<-resid(lm.M2)/sum2$sigma 
names(regdata2)
head(regdata2)
```

Class note: Jump to new R-squared, we get a slightly better value. See if you can play around with the other components/variables. 

Get diagnostic plots

```{r}
#create a 2x2 plot so the next four appear together
par(mfrow=c(2,2),mai=c(0.6,0.6,0.6,0.6),cex=0.7)

#residuals vs fitted values plot, to check for linearity
plot(regdata2$yhat.M2,regdata2$resid.M2, main="Model 2, Residual Plot",
  xlab="yhat", ylab="residual")
abline(0,0)
# note: the points seem scattered in the plot, so there is no linearity

#get the min and max values for fat to set axes equally in the next plot
minfat<-min(regdata2$fat)
maxfat<-max(regdata2$fat)

#predicted vs measured plot, to compare model predictions of observed fat
plot(regdata2$fat,regdata2$yhat.M2, main="Model 2, Fitted line plot",
  ylab="yhat", xlab="fat",ylim=c(minfat,maxfat),xlim=c(minfat,maxfat))
abline(0,1)
#note: points are close to the 1:1 line in a slightly linear pattern

#Q-Q plot of residuals, checks if the residuals are normally distributed
qqnorm(regdata2$std.resid.M2, main="Model 2, Normality plot")
abline(0,1)
#note: points more or less fall along the axis, so normality is reasonable

#historgram of residuals, to check the shape of error distribution
hist(regdata2$resid.M2, breaks =8 , density=10,col="green", border="black",
main="Model 2, Error Distribution") 
#note: errors are normally distributed, with a bell shape pattern 

```

Note: they look a little better than before. The normality plot is sitting more on the line. The error distriubtion is much better than before. 

____
#### Part I: First Approach

**Answer QUESTION 1 - 5 above.**
***Answers are above***
----

#### Part II: Second Approach

**Answer QUESTION 6 - 9 below**

**QUESTION 6: Using the first three principal components and the correlations between these and the original X variables, choose a name for each of these principal components (i.e. a label that describes each component). Briefly describe your choice for labels.**

```{r}
# provide your R codes here (if possible)

cor_table #regression table

```

**Question 6 Answer**
***PC1 - Overall Body Size. This is because all weight, height, length, and
strength variables are strongly and positively related to this
component. This indicates that PC1 captures general growth variation
across all ages.***

***PC2 - Strength vs Body Mass. This is because strength and height at
older ages are positively related, while weight and length are
negatively related. This suggests that PC2 reflects differences in body
composition and physical performance rather than overall size.***

***PC3 - appears to capture additional age-specific growth variation not
explained by the first two components. It may represent more subtle
developmental differences across ages.***

----

**QUESTION 7: Using EXCEL or R code, calculate the principal component scores for all observations for principal component 1 only. Show how you did the calculations in your report and give the answers. Careful!! The default is to use the correlation matrix for the PCA using `princomp()` in R. As a result, you will need to normalize the X variables (subtract the mean and then divide by the standard deviation) before using them to calculate the principal component scores!!**

#Class note: need to calculate the loadings (eigenvector matrix) and multiply by the original dataset, and then get the principal components. 

```{r}
# provide your R codes here (if possible)

## Inputs we already have:
##   xdata      = the original X-only data frame 
##   xdata.pca  = the princomp results

#Pull the PC1 scores directly from the princomp xdata.pca object to compare with later: 

pc1scores7 <- xdata.pca$scores[, 1]
pc1scores7

pc1scores7_table <- data.frame(obs = seq_along(pc1scores7), PC1_score = as.numeric(pc1scores7))
pc1scores7_table

#Now to manually create the matrix and calculate eigenvector loadings

# X matrix (original variables)
X <- as.matrix(xdata)

# Create dfs for means and SDs used by princomp for standardization
mu  <- xdata.pca$center
sdv <- xdata.pca$scale

# Standardize X to z-scores: Z_ij = (X_ij - mean_j) / sd_j
Z <- sweep(X, 2, mu, "-")
Z <- sweep(Z, 2, sdv, "/")

# Create the eigenvector matrix (loadings) where columns are eigenvectors for all PCs
V <- as.matrix(xdata.pca$loadings)

# Filter for the first PC1 eigenvector (first column of the eigenvector matrix)
w1 <- V[, 1]

# Compute PC1 scores for all observations
pc1_manual <- as.numeric(Z %*% w1)

# Store manual results
pc1_scores_table_7 <- data.frame(
  obs = seq_len(nrow(Z)),
  PC1_score_manual = pc1_manual
)
pc1_scores_table_7

# Compare manual vs R scores (should match)
comparison_table <- data.frame(
  obs = seq_len(nrow(Z)),
  PC1_manual = pc1_manual,
  PC1_R = as.numeric(pc1scores7),
  diff = pc1_manual - as.numeric(pc1scores7)
)

comparison_table
cor(comparison_table$PC1_manual, comparison_table$PC1_R)


pc1_R <- as.numeric(xdata.pca$scores[, 1])
pc1_R_table <- data.frame(obs = seq_along(pc1_R), PC1_score_R = pc1_R)

#Note: the scores are the same from the printcomp object! Yay!

```

**Question 7 Answer**
***We used R codes. The principal component scores for PC1 were calculated by standardizing the original X variables and multiplying the standardized matrix by the PC1 loadings. The manually calculated scores were identical to the scores generated directly by R.*** 

----

**QUESTION 8: How do your answers in QUESTION 7 compare to the values calculated by R? Show the comparison results in your report and use that in answering this question.**

```{r}
# provide your R codes here (if possible)

#Compare manual vs R scores (should match)
comparison_table <- data.frame(
  obs = seq_len(nrow(Z)),
  PC1_manual = pc1_manual,
  PC1_R = as.numeric(pc1scores7),
  diff = pc1_manual - as.numeric(pc1scores7)
)

#double check against the printcomp object
comparison_table
cor(comparison_table$PC1_manual, comparison_table$PC1_R)

#correlation check, just in case - should be 1, and it is! Yay! 
cor(comparison_table$PC1_manual, comparison_table$PC1_R)

```

**Question 8 Answer**

***Comparing the principal component 1 scores calculated manually in Question 7, the scores are identical to those produced by R, with no numerical differences between the two sets. This exact match shows that the manual calculations with standardization was able to replicate R’s internal PCA scoring procedure. The agreement is expected because the same standardized X variables and the same PC1 loading vector (eigenvector) were used in both approaches.***

----

**QUESTION 9: Using the regression of `fat` versus the first three principal components:**

**a. What is the R2 value for this regression with the reduced set of variables? What does this mean? Is this a good model?**

**b. Look at the fitted line plot, showing the predicted `fat` (from the regression equation) vs. the measured `fat`. Is this a good relationship?**

**c. Since the PC’s are uncorrelated and you have given a label for each PC, you can interpret the regression coefficients. What happens to `fat` when the first principal component scored increases (CAREFUL!! There may be some negative and/or positive correlations between the x-variables and the principal components? What does this mean based on the labels you chose for the first principal component?**

**d. Repeat #9c using the second and then the third principal component.**

```{r}
# provide your R codes here (if possible)

#Extract the 3 PC scores
lpc_scores <- xdata.pca$scores[, 1:3] 

#add them to fatdata, and check
fatdat$PC1 <- lpc_scores[,1] 
fatdat$PC2 <- lpc_scores[,2]
fatdat$PC3 <- lpc_scores[,3]
names(fatdat)

#Fit the model
lm.M2 <- lm(fat ~ PC1 + PC2 + PC3, data = fatdat)
summary(lm.M2)

fatdat$yhat.M2 <- fitted(lm.M2)
fatdat$resid.M2 <- resid(lm.M2)
fatdat$std.resid.M2 <- resid(lm.M2) / summary(lm.M2)$sigma

par(mfrow=c(2,2), mai=c(0.6,0.6,0.6,0.6), cex=0.7)

# Create the residual plot
plot(fatdat$yhat.M2, fatdat$resid.M2,
     main="Model 2, Residual Plot",
     xlab="Predicted fat", ylab="Residual")
abline(0,0)

# Fitted vs measured plot
plot(fatdat$fat, fatdat$yhat.M2,
     main="Model 2, Fitted Line Plot",
     xlab="Measured fat", ylab="Predicted fat")
abline(0,1)

# Normal Q-Q plot
qqnorm(fatdat$std.resid.M2,
       main="Model 2, Normality Plot")
abline(0,1)

# Histogram
hist(fatdat$resid.M2,
     breaks=8,
     col="green",
     border="black",
     main="Model 2, Error Distribution")


```

**Question 9 Answer**

Note: look at coefficients in c, which is the intercept + ... (this is the starting point). How do you explain these coefficients.  

**a. The $R^2$ value is 0.6856. This means that 68.56% of the variance in fat is explained by PC1, PC2, and PC3. This is a good model, and it improves on the previous regression using the correlated X variables (which had $R^2$ = 0.587). The overall model is statistically significant (p \< 0.001).**

**b. Yes, this is a good relationship. In the fitted line plot (predicted fat vs. measured fat), the points generally follow the 45° reference line, there is a clear linear trend, and the spread around the line is moderate and does not show obvious curvature. This indicates that the regression model is doing a reasonably good job predicting fat. While there is some variability around the line (which is expected), the overall linear relationship between predicted and measured fat is strong and appropriate for a linear model. Therefore, the fitted line plot supports that this is a good linear fit.**

**c. The coefficient for PC1 is positive (0.13739) and statistically significant. Since PC1 represents overall physical development (all size and strength variables are positively related to it), this means that as PC1 increases, fat increases. Therefore, individuals with greater overall body size and strength tend to have higher fat levels.**

**d. PC2 has a negative and highly significant coefficient (-0.46599), meaning that as PC2 increases (representing relatively greater strength and lower body mass), fat decreases. PC3 has a negative but non-significant coefficient (-0.11457), indicating that it does not significantly contribute to predicting fat in this model.**


----

#### Part III: Overall Summary

**Answer QUESTION 10 below**

**QUESTION 10: Which approach did you like better for predicting `fat` using PCA as a first step before the regression analysis? Why? Would you have continued with three principal components or not? Give a brief report (half a page to 1 page) explaining your preferences with evidence to support these.**

**Question 10 Answer**

Class note: Compare the models and strengths. PCA is easier and coefficients is much better (not immensely) but error models looked to better explain datasets. Use the PCA to select the x variables to keep for later analysis. But justify which ones. 

These two models were used to predict body fat. To summarize outputs: The first model in Part 1 reduced the original x values based on PCA results and then fitted a regression model. This model used PCA to explore variable reductions. In the first part, ~52% of total variance was explained by the first PC and 83% amongst the first 3, with PC1 showing high correlations with all body size variables, with interpretations focused on a size or growth pattern emerging from PC1. So, model reduction using the first 3 PCs was chosen to reasonably eliminate redundancy. Subsequent regression showed that ~59% of the variance in fat (r = 0.54) was significant (p < 0.001), with height and weight being particularly dominant. Length at age 9, however, was not significant. So, this implies that there was some remaining redundancy. The resulting diagnostic plots were not perfect, though still showed reasonably well-behaved residuals. The plots showed residuals centered around 0 with no extreme deviations, but the variability around the fitted line suggested that prediction accuracy could be improved. Overall, this model seemed like a fine starting point, but could be refined. 

In Part 2, PC scores were used as predictors in regression. In this model, the regression coefficients were used as the main factor in interpretation. Given that the first 3 PCs together explained 83% of the variance, with the scree plot offering visual agreement, these first three PCs were used. The resulting regression produced a slightly higher r value than the first model, and exhibited stronger performance overall, which suggested better explanatory performance. The diagnostic plots showed residuals fitted close to 0 in the fitted linear plot, and there was a better "curve" in the error distribution plot. Overall, this model offered more refinement, though it admittedly did make interpretation a little less intuitive than the first model, as we really needed to read between the lines. Regardless, this PCA score-based model performed better statistically. Patterns suggested that PC1 hinted at differences between late-age and early-age size, though PC3 (early-age variables) made that hard to detect given the smaller loading (though we could still say that there was at least a minimal pattern here).      

In terms of selecting the model we liked more, there were pros and cons associated with both. The first model seemed far less complex, and its straightforwardness lended to easier-to-interpret results. However, it was not as statistically sound as the second model. The second model was certainly more cumbersome in terms of thinking it through, but the results spoke for themselves. This second model seemed to take better care of colinearity (since it used PCA scores directly), and provided better model fit and cleaner diagnostics. In sum, model 1 performed adequately and was simpler, while model 2 performed better statistically with more optimal outputs but with more careful interpretation. Overall, we are inclined to choose model 2. 

In general, we think keeping the first 3 PCs was appropriate in both cases, since they captured the majority of meaningful variation and produced more stable regression models. If the point of PCA is to reduce dimensions and make interpretability easier, keeping the first three PCs seems like the optimal choice to balance explanatory power and simplicity. It is possible that adding other PCs could reveal more subtle patterns. However, the scree plots showed that the PCs after PC3 contributed relatively little information, with only the first handful of components clearly carrying the most predictive power. Additional components would likely add more noise than improved model performance. 

An interesting question might be whether restricting the models to just the first 2 PCs would reveal clearer interpretations or stronger model performance. PC3 explained a much smaller proportion of variance (~12%) compared to PC1 (~52%) and PC2 (~18%) and its contribution to predicting fat was noticeably weaker than the first two PCs. However, PC3 still had a moderate effect in these models, and it still captured variation related to early-age body size. For us, this justifies keeping it for future iterations, in order to retain overall model fit while keeping a high degree of variance. 

Reducing models to only the first 2 PCs could be compelling depending on the research question, or if the goal was to further optimize interpretability and conciseness. It might be the case that there is little loss of predictive accuracy when PC3 is excluded (and additional comparisons could be easily pursued). But based on the results here, PC3 does have explanatory value and we believe should be retained. 
